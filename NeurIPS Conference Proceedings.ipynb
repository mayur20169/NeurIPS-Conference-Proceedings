{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0CSpPDmZ-Nm"
      },
      "outputs": [],
      "source": [
        "# NeurIPS Conference Proceedings\n",
        "# Author Name : Mayur Mankar\n",
        "# Roll No. : 20169\n",
        "# Department : Data Science and Engineering\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4\n",
        "import lxml\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "response = requests.get(\"https://proceedings.neurips.cc/\")\n",
        "soup = bs4.BeautifulSoup(response.content, \"lxml\")\n",
        "link_of_pages = []\n",
        "l1 = soup.find_all(\"a\")\n",
        "\n",
        "for link in l1:\n",
        "  some_text = link.get('href')\n",
        "  if some_text:\n",
        "    if \"paper\" in some_text:\n",
        "      link_of_pages.append(link.get('href'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbwOECTm5sVA"
      },
      "outputs": [],
      "source": [
        "out_url = []\n",
        "for link in link_of_pages:\n",
        "    out_url.append(\"https://proceedings.neurips.cc/\" + link)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p-EuqOE6QlM"
      },
      "outputs": [],
      "source": [
        "# iterate over the list and print the links and store them in a list\n",
        "list_of_html = []\n",
        "\n",
        "for link in out_url:\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content, 'html5lib')\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    for link in links:\n",
        "      some_text = link.get('href')\n",
        "      if some_text:\n",
        "        if \"html\" in some_text:\n",
        "          list_of_html.append(link.get('href'))\n",
        "\n",
        "# store the links in a csv file\n",
        "df = pd.DataFrame(list_of_html,columns = ['PAPER LINK'])\n",
        "df.to_csv('list_of_html.csv', index = True, index_label = 'ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9YjfO5Ab0Sy"
      },
      "outputs": [],
      "source": [
        "paper_list = []\n",
        "list_of_papers = []\n",
        "list_of_authors = []\n",
        "\n",
        "for link in out_url:\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content, 'html5lib')\n",
        "    all_links = soup.find_all('li')\n",
        "    \n",
        "    for link in all_links:\n",
        "      if \"logout\" not in str(link) and \"login\" not in str(link) :\n",
        "        paper_list.append(str(link))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqoCfuCJuJys"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "list_of_papers = []\n",
        "list_of_authors = []\n",
        "\n",
        "r1 = re.compile('html\">(.*?)</a>')\n",
        "r2 = re.compile('<i>(.*?)</i>')\n",
        "\n",
        "for i in paper_list:\n",
        "  m1 = r1.search(i)\n",
        "  m2 = r2.search(i)\n",
        "  if m1:\n",
        "    list_of_papers.append(m1.group(1))\n",
        "  if m2:\n",
        "    list_of_authors.append(m2.group(1))\n",
        "\n",
        "df2 = pd.DataFrame (list_of_papers, columns = ['PAPER NAME'])\n",
        "df2.to_csv('list_of_papers.csv', index = True, index_label = 'ID')\n",
        "\n",
        "df3 = pd.DataFrame (list_of_authors, columns = ['AUTHORS NAME'])\n",
        "df3.to_csv('list_of_authors.csv', index = True, index_label = 'ID')\n",
        "\n",
        "data2_import = pd.read_csv('list_of_papers.csv')           \n",
        "data3_import = pd.read_csv('list_of_authors.csv')\n",
        "\n",
        "data_merge1 = pd.merge(data2_import,data3_import,on = \"ID\",how = \"outer\")  \n",
        "data_merge1.to_csv('list_of_paper_and_author_name.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ux_0jVFiO7v"
      },
      "outputs": [],
      "source": [
        "data4_import = pd.read_csv('list_of_paper_and_author_name.csv') \n",
        "data5_import = pd.read_csv('list_of_html.csv')\n",
        "\n",
        "data_merge2 = pd.merge(data4_import,data5_import,on = \"ID\",how = \"outer\")  \n",
        "data_merge2.to_csv('DSE_20169_Mayur-Mankar_Assignment1WS.csv', index = False,encoding = \"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3054_IuXzeFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}